upstream vllm_backends {
    # Replace these placeholders with the actual IP addresses and ports
    server 192.168.1.1:8000;  # Your first vLLM instance
    server 192.168.1.8:8000;  # Your second vLLM instance
    
    # Optional: Use a specific load balancing method if needed, e.g. least_conn, ip_hash, hash $request_uri, random,
    least_conn;
}


server {
    listen 80;
    server_name _;

    root /mnt/data/office_work/vllms_inference;
    index chatbot.html;

    # Existing location for serving the static front-end files (HTML, JS, CSS)
    location / {
        try_files $uri $uri/ =404;
    }

    # ðŸ‘‡ NEW: Location for the vLLM API traffic (e.g., /v1/completions) ðŸ‘‡
    location /v1/ {
        # Forward the request to the pool of vLLM servers
        proxy_pass http://vllm_backends;

        # Set headers for proper request handling by the backend vLLM server
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Crucial for LLM streaming responses, prevents connection timeouts
        proxy_read_timeout 600s;
        proxy_send_timeout 600s;
    }
}
